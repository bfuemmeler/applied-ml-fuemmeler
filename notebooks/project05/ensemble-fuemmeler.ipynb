{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ed9919a",
   "metadata": {},
   "source": [
    "## Ensemble Machine Learning P5\n",
    "\n",
    "# Ensemble ML, Spiral with Wine\n",
    "# **Author:** Brenda Fuemmeler\n",
    "# **Date:** November 21, 2025\n",
    "# **Objective:** Use Ensemble Models to improve performance over individual models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ec453e",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This project uses the Wine dataset to show how ensemble models can perform as compared to individual models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeaae53d",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45d81f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    AdaBoostClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    BaggingClassifier,\n",
    "    VotingClassifier,\n",
    ")\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030c1096",
   "metadata": {},
   "source": [
    "## Section 1. Load and Inspect the data\n",
    "\n",
    "### 1.1 Load the Wine dataset and display the first 10 rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3323a304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1599 entries, 0 to 1598\n",
      "Data columns (total 12 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   fixed acidity         1599 non-null   float64\n",
      " 1   volatile acidity      1599 non-null   float64\n",
      " 2   citric acid           1599 non-null   float64\n",
      " 3   residual sugar        1599 non-null   float64\n",
      " 4   chlorides             1599 non-null   float64\n",
      " 5   free sulfur dioxide   1599 non-null   float64\n",
      " 6   total sulfur dioxide  1599 non-null   float64\n",
      " 7   density               1599 non-null   float64\n",
      " 8   pH                    1599 non-null   float64\n",
      " 9   sulphates             1599 non-null   float64\n",
      " 10  alcohol               1599 non-null   float64\n",
      " 11  quality               1599 non-null   int64  \n",
      "dtypes: float64(11), int64(1)\n",
      "memory usage: 150.0 KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 798 entries, 0 to 797\n",
      "Data columns (total 1 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   A\tB\tClass  798 non-null    object\n",
      "dtypes: object(1)\n",
      "memory usage: 6.4+ KB\n",
      "                    A\\tB\\tClass\n",
      "0  3.912030016\\t-1.108530754\\t0\n",
      "1   2.663917629\\t2.714674035\\t0\n",
      "2   0.481765126\\t0.088642555\\t0\n",
      "3   -0.83924748\\t3.163378985\\t0\n",
      "4  3.915365837\\t-0.939925098\\t0\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset (download from UCI and save in the same folder)\n",
    "df = pd.read_csv(\"winequalityred.csv\", sep=\";\")\n",
    "\n",
    "# Display structure and first few rows\n",
    "df.info()\n",
    "df.head()\n",
    "\n",
    "# The dataset includes 11 physicochemical input variables (features):\n",
    "# ---------------------------------------------------------------\n",
    "# - fixed acidity          mostly tartaric acid\n",
    "# - volatile acidity       mostly acetic acid (vinegar)\n",
    "# - citric acid            can add freshness and flavor\n",
    "# - residual sugar         remaining sugar after fermentation\n",
    "# - chlorides              salt content\n",
    "# - free sulfur dioxide    protects wine from microbes\n",
    "# - total sulfur dioxide   sum of free and bound forms\n",
    "# - density                related to sugar content\n",
    "# - pH                     acidity level (lower = more acidic)\n",
    "# - sulphates              antioxidant and microbial stabilizer\n",
    "# - alcohol                % alcohol by volume\n",
    "\n",
    "# The target variable is:\n",
    "# - quality (integer score from 0 to 10, rated by wine tasters)\n",
    "\n",
    "# We will simplify this target into three categories:\n",
    "#   - low (3–4), medium (5–6), high (7–8) to make classification feasible.\n",
    "#   - we will also make this numeric (we want both for clarity)\n",
    "# The dataset contains 1599 samples and 12 columns (11 features + target).\n",
    "\n",
    "\n",
    "\n",
    "# Load spiral dataset\n",
    "spiral = pd.read_csv(\"spiral.csv\")\n",
    "\n",
    "# Display basic information\n",
    "spiral.info()\n",
    "\n",
    "# Display first few rows\n",
    "print(spiral.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bcdbbc",
   "metadata": {},
   "source": [
    "## Section 2. Prepare the Data\n",
    "# Includes cleaning, feature engineering, encoding, splitting, helper functions\n",
    "# In this section we will take the numeric wine quality score and convert it into two new forms: a text label and a numeric category. This will help us interpret and visualize the data in our models going forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e935e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper function that:\n",
    "\n",
    "# Takes one input, the quality (which we will temporarily name q while in the function)\n",
    "# And returns a string of the quality label (low, medium, high)\n",
    "# This function will be used to create the quality_label column\n",
    "def quality_to_label(q):\n",
    "    if q <= 4:\n",
    "        return \"low\"\n",
    "    elif q <= 6:\n",
    "        return \"medium\"\n",
    "    else:\n",
    "        return \"high\"\n",
    "\n",
    "\n",
    "# Call the apply() method on the quality column to create the new quality_label column\n",
    "df[\"quality_label\"] = df[\"quality\"].apply(quality_to_label)\n",
    "\n",
    "\n",
    "# Then, create a numeric column for modeling: 0 = low, 1 = medium, 2 = high\n",
    "def quality_to_number(q):\n",
    "    if q <= 4:\n",
    "        return 0\n",
    "    elif q <= 6:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "\n",
    "df[\"quality_numeric\"] = df[\"quality\"].apply(quality_to_number)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0754bbe",
   "metadata": {},
   "source": [
    "## Section 3. Feature Selection and Justification\n",
    "# First we need to define the x & y input features. Then we will drop all columns except for Quality, Quality Label and Quality Numeric. This will ensure the model is trained to predict wine quality by only using the chemical measurements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2793e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input features (X) and target (y)\n",
    "# Features: all columns except 'quality' and 'quality_label' and 'quality_numberic' - drop these from the input array\n",
    "# Target: quality_label (the new column we just created)\n",
    "X = df.drop(columns=[\"quality\", \"quality_label\", \"quality_numeric\"])  # Features\n",
    "y = df[\"quality_numeric\"]  # Target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9c25c5",
   "metadata": {},
   "source": [
    "## Section 4. Split the Data into Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f775179a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split (stratify to preserve class balance)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95721c03",
   "metadata": {},
   "source": [
    "## Section 5.  Evaluate Model Performance (Choose 2)\n",
    "# 1 Random Forest (100)\tA strong baseline model using 100 decision trees.\n",
    "# 2 Bagging (DT, 100)\tBuilds many trees in parallel on different samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f73ab45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to train and evaluate models\n",
    "def evaluate_model(name, model, X_train, y_train, X_test, y_test, results):\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    train_acc = accuracy_score(y_train, y_train_pred)\n",
    "    test_acc = accuracy_score(y_test, y_test_pred)\n",
    "    train_f1 = f1_score(y_train, y_train_pred, average=\"weighted\")\n",
    "    test_f1 = f1_score(y_test, y_test_pred, average=\"weighted\")\n",
    "\n",
    "    print(f\"\\n{name} Results\")\n",
    "    print(\"Confusion Matrix (Test):\")\n",
    "    print(confusion_matrix(y_test, y_test_pred))\n",
    "    print(f\"Train Accuracy: {train_acc:.4f}, Test Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"Train F1 Score: {train_f1:.4f}, Test F1 Score: {test_f1:.4f}\")\n",
    "\n",
    "    results.append(\n",
    "        {\n",
    "            \"Model\": name,\n",
    "            \"Train Accuracy\": train_acc,\n",
    "            \"Test Accuracy\": test_acc,\n",
    "            \"Train F1\": train_f1,\n",
    "            \"Test F1\": test_f1,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee472ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forest (100) Results\n",
      "Confusion Matrix (Test):\n",
      "[[  0  13   0]\n",
      " [  0 256   8]\n",
      " [  0  15  28]]\n",
      "Train Accuracy: 1.0000, Test Accuracy: 0.8875\n",
      "Train F1 Score: 1.0000, Test F1 Score: 0.8661\n"
     ]
    }
   ],
   "source": [
    "# 1. Random Forest\n",
    "results = []\n",
    "\n",
    "evaluate_model(\n",
    "    \"Random Forest (100)\",\n",
    "    RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    results,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280f8e82",
   "metadata": {},
   "source": [
    "# Explanation of Random Forest results: \n",
    "Row 1 Actual Low-Quality: [0 13 0]\n",
    "0 predicted correctly\n",
    "13 mislabeled as medium\n",
    "0 mislabeled as high\n",
    "\n",
    "Row 2 Actual Medium-Quality: [0 256 8]\n",
    "256 correctly predicted\n",
    "8 mislabeled as high\n",
    "\n",
    "Row 3 Actual High-Quality: [0 15 28]\n",
    "28 predicted correctly\n",
    "15 mislabeled as medium\n",
    "\n",
    "This Confusion Matrix reads heavily on the medium class. It reasonably identifies high-quality, while completely fails at catching low-quality wines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f01bee81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bagging (DT, 100) Results\n",
      "Confusion Matrix (Test):\n",
      "[[  0  13   0]\n",
      " [  0 252  12]\n",
      " [  0  12  31]]\n",
      "Train Accuracy: 1.0000, Test Accuracy: 0.8844\n",
      "Train F1 Score: 1.0000, Test F1 Score: 0.8655\n"
     ]
    }
   ],
   "source": [
    "# 8. Bagging\n",
    "results = []\n",
    "\n",
    "evaluate_model(\n",
    "    \"Bagging (DT, 100)\",\n",
    "    BaggingClassifier(\n",
    "        estimator=DecisionTreeClassifier(), n_estimators=100, random_state=42\n",
    "    ),\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    results,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f666c9d",
   "metadata": {},
   "source": [
    "# Explanation of Bagging results: \n",
    "Row 1 Actual Low-Quality: [0 13 0]\n",
    "0 predicted correctly\n",
    "13 mislabeled as medium\n",
    "0 mislabeled as high\n",
    "\n",
    "Row 2 Actual Medium-Quality: [0 252 12]\n",
    "252 correctly predicted\n",
    "12 mislabeled as high\n",
    "\n",
    "Row 3 Actual High-Quality: [0 12 31]\n",
    "12 predicted correctly\n",
    "31 mislabeled as medium\n",
    "\n",
    "This model also strongly favors the medium class. It performed poorly on low-quality wines, while performing moderately on high-quality wines. Most mistakes center around predicting medium instead of low or high."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c56d24",
   "metadata": {},
   "source": [
    "## Section 6. Compare Results\n",
    "Random Forest (100) and Bagging both showed similar patterns, but Random Forest performed better overall. Both models had difficulty predicting low-quality wines, predicting none of the samples correctly and misclassifying all of them as medium. Random Forest resulted in higher accuracy on the medium and high-classes. \n",
    "Random Forest also shows stronger overall test metrics, showing higher accuracy and F1-score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef81df6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary of All Models (Sorted by Test Accuracy):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Train Accuracy</th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>Train F1</th>\n",
       "      <th>Test F1</th>\n",
       "      <th>Accuracy Gap</th>\n",
       "      <th>F1 Gap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest (100)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.887500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.547722</td>\n",
       "      <td>0.112500</td>\n",
       "      <td>0.452278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bagging (DT, 100)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.884375</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.550846</td>\n",
       "      <td>0.115625</td>\n",
       "      <td>0.449154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model  Train Accuracy  Test Accuracy  Train F1   Test F1  \\\n",
       "0  Random Forest (100)             1.0       0.887500       1.0  0.547722   \n",
       "1    Bagging (DT, 100)             1.0       0.884375       1.0  0.550846   \n",
       "\n",
       "   Accuracy Gap    F1 Gap  \n",
       "0      0.112500  0.452278  \n",
       "1      0.115625  0.449154  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a table of results\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import pandas as pd\n",
    "\n",
    "# Create a list to store results\n",
    "results = []\n",
    "\n",
    "# ----- 1. Bagging (Decision Trees) -----\n",
    "bagging = BaggingClassifier(n_estimators=100, random_state=42)\n",
    "bagging.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_bag = bagging.predict(X_train)\n",
    "y_test_pred_bag = bagging.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "train_acc_bag = accuracy_score(y_train, y_train_pred_bag)\n",
    "test_acc_bag = accuracy_score(y_test, y_test_pred_bag)\n",
    "train_f1_bag = f1_score(y_train, y_train_pred_bag, average='macro')\n",
    "test_f1_bag = f1_score(y_test, y_test_pred_bag, average='macro')\n",
    "\n",
    "results.append({\n",
    "    \"Model\": \"Bagging (DT, 100)\",\n",
    "    \"Train Accuracy\": train_acc_bag,\n",
    "    \"Test Accuracy\": test_acc_bag,\n",
    "    \"Train F1\": train_f1_bag,\n",
    "    \"Test F1\": test_f1_bag,\n",
    "    \"Accuracy Gap\": train_acc_bag - test_acc_bag,\n",
    "    \"F1 Gap\": train_f1_bag - test_f1_bag\n",
    "})\n",
    "\n",
    "# ----- 2. Random Forest -----\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_rf = rf.predict(X_train)\n",
    "y_test_pred_rf = rf.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "train_acc_rf = accuracy_score(y_train, y_train_pred_rf)\n",
    "test_acc_rf = accuracy_score(y_test, y_test_pred_rf)\n",
    "train_f1_rf = f1_score(y_train, y_train_pred_rf, average='macro')\n",
    "test_f1_rf = f1_score(y_test, y_test_pred_rf, average='macro')\n",
    "\n",
    "results.append({\n",
    "    \"Model\": \"Random Forest (100)\",\n",
    "    \"Train Accuracy\": train_acc_rf,\n",
    "    \"Test Accuracy\": test_acc_rf,\n",
    "    \"Train F1\": train_f1_rf,\n",
    "    \"Test F1\": test_f1_rf,\n",
    "    \"Accuracy Gap\": train_acc_rf - test_acc_rf,\n",
    "    \"F1 Gap\": train_f1_rf - test_f1_rf\n",
    "})\n",
    "\n",
    "# ----- 3. Create summary table -----\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Sort by Test Accuracy (descending)\n",
    "results_df = results_df.sort_values(by=\"Test Accuracy\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"\\nSummary of All Models (Sorted by Test Accuracy):\")\n",
    "display(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2994b5c2",
   "metadata": {},
   "source": [
    "## Section 7. Conclusions and Insights\n",
    "Both models I used achieved perfect training accuracy of 1.0. When comparing some models that my peers used, I noticed that other models resulted in lower training accuracy. While the test accuracy on my models is very close, Random Forest (100) achieved the best. In comparison to my peers, I really did not see other models that performed higher. I chose to include Accuracy Gap and F1 Gap in my model summary. The Accuracy Gap and F1 Gap for both was relatively small, which suggest some overfitting. These models suggest some class imbalance in our dataset, which can cause misclassification. These models favored the class with the majority of results (medium), and did not show a true performance of the class with fewest results (low). These ensemble methods were very interesting to use. They clearly show that models can be easily misrepresented if there is class imbalance in the dataset. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "applied-ml-fuemmeler",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
